import streamlit as st
import pandas as pd
import plotly.express as px
import FinanceDataReader as fdr
import requests
import urllib3
from io import StringIO, BytesIO
from datetime import datetime, timedelta
import yfinance as yf
import feedparser
import numpy as np
import pytz
import sqlite3
from collections import defaultdict
import math

# [í•„ìˆ˜] ê°™ì€ í´ë”ì˜ etf.pyì—ì„œ í´ë˜ìŠ¤ ì„í¬íŠ¸
try:
    from etf import ActiveETFMonitor
except ImportError:
    st.error("âš ï¸ 'etf.py' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”.")
    st.stop()

# ë³´ì•ˆ ì¸ì¦ì„œ ê²½ê³  ë¬´ì‹œ ë° SSL ê²€ì¦ ìš°íšŒ (Global Patch)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
original_request = requests.Session.request
def patched_request(self, method, url, *args, **kwargs):
    kwargs['verify'] = False
    return original_request(self, method, url, *args, **kwargs)
requests.Session.request = patched_request


# ---------------------------------------------------------
# 1. í˜ì´ì§€ ì„¤ì •
# ---------------------------------------------------------
st.set_page_config(
    page_title="MAS Strategy Dashboard",
    page_icon="ğŸŠ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ---------------------------------------------------------
# 2. ë°ì´í„° ìˆ˜ì§‘ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------------------------------------

@st.cache_data(ttl=600)
def fetch_market_data():
    """ì‹œì¥ í•µì‹¬ ì§€í‘œ ìˆ˜ì§‘"""
    tickers = {
        "KOSPI": "^KS11", "S&P500": "^GSPC", "Nasdaq": "^IXIC", 
        "USD/KRW": "KRW=X", "US 10Y": "^TNX", "WTI Oil": "CL=F"
    }
    data_dict = {}
    history_dict = {}
    
    for name, code in tickers.items():
        try:
            obj = yf.Ticker(code)
            hist = obj.history(period="1y")
            if not hist.empty:
                current = hist['Close'].iloc[-1]
                prev = hist['Close'].iloc[-2]
                pct_change = ((current - prev) / prev) * 100
                hist['MA20'] = hist['Close'].rolling(window=20).mean()
                trend = "ìƒìŠ¹" if current > hist['MA20'].iloc[-1] else "í•˜ë½"
                data_dict[name] = {"price": current, "pct_change": pct_change, "trend": trend}
                history_dict[name] = hist
        except: continue
    return data_dict, history_dict

def to_excel(df_new, df_inc, df_dec, df_all, date):
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_new.to_excel(writer, index=False, sheet_name='ì‹ ê·œí¸ì…')
        df_inc.to_excel(writer, index=False, sheet_name='ë¹„ì¤‘í™•ëŒ€')
        df_dec.to_excel(writer, index=False, sheet_name='ë¹„ì¤‘ì¶•ì†Œ')
        df_all.to_excel(writer, index=False, sheet_name='ì „ì²´í¬íŠ¸í´ë¦¬ì˜¤')
    return output.getvalue()



def fetch_yahoo_news(tickers):
    """Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ (ë” ì‹ ë¢°ë„ ë†’ì€ ì†ŒìŠ¤)"""
    news_items = []
    try:
        # ì—¬ëŸ¬ í‹°ì»¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
        for ticker in tickers:
            stock = yf.Ticker(ticker)
            news = stock.news
            if news:
                for n in news:
                    # YF ë‰´ìŠ¤ êµ¬ì¡°: title, link, providerPublishTime, publisher
                    pub_time = n.get('providerPublishTime', 0)
                    dt = datetime.fromtimestamp(pub_time)
                    
                    news_items.append({
                        "title": n.get('title', ''),
                        "link": n.get('link', ''),
                        "published_dt": dt,
                        "published": dt.strftime("%Y-%m-%d %H:%M"),
                        "source": f"Yahoo ({n.get('publisher', 'Unknown')})"
                    })
    except Exception as e:
        # st.error(f"Yahoo News Error: {e}") # ë””ë²„ê¹…ìš©
        pass
        
    return news_items

@st.cache_data(ttl=3600)
def fetch_global_events():
    """ì „ì²´ ì‹œì¥ í•µì‹¬ ì´ë²¤íŠ¸ ìˆ˜ì§‘ (Google News + Yahoo Finance)"""
    market_news = []
    
    # 1. Yahoo Finance (ì‹ ë¢°ì˜¤ ì†ŒìŠ¤ ìš°ì„  - SPY, QQQ, NVDA)
    market_news.extend(fetch_yahoo_news(["SPY", "QQQ", "^DJI"]))
    
    # 2. Google News (ë³´ì¡°)
    # ê´‘ë²”ìœ„í•œ ì‹œì¥ í‚¤ì›Œë“œ
    query = "stock market live updates Fed CPI inflation earnings report when:3d"
    encoded = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(url)
        for e in feed.entries:
            # ë‚ ì§œ íŒŒì‹±
            if hasattr(e, 'published_parsed') and e.published_parsed:
                dt = datetime(*e.published_parsed[:6])
            else:
                dt = datetime.now()

            market_news.append({
                "title": e.title,
                "link": e.link,
                "published": e.published,
                "published_dt": dt, # ì •ë ¬ìš©
                "source": e.source.title if hasattr(e, 'source') else "News"
            })
    except: pass
    
    # ì¤‘ë³µ ì œê±° (Link ê¸°ì¤€) & ì •ë ¬
    seen_links = set()
    unique_news = []
    for n in market_news:
        if n['link'] not in seen_links:
            unique_news.append(n)
            seen_links.add(n['link'])
            
    # ìµœì‹ ìˆœ ì •ë ¬
    unique_news.sort(key=lambda x: x['published_dt'], reverse=True)
    
    return unique_news[:7] # Top 7 (ì•¼í›„ ì¶”ê°€ë¡œ ê°œìˆ˜ ëŠ˜ë¦¼)


# =========================
# KDI-style Issue Trend MVP
# =========================

ISSUE_DB_PATH = "issue_trend.db"

# ìì‚°ë°°ë¶„ ê´€ì  ì´ìŠˆ ì„¸íŠ¸ (MVP 12ê°œ)
ISSUES = {
    "ë¬¼ê°€/ì¸í”Œë ˆ": {
        "kw": ["cpi", "pce", "inflation", "disinflation", "core", "headline", "prices", "ë¬¼ê°€", "ì¸í”Œë ˆ", "ì¸í”Œë ˆì´ì…˜", "ê·¼ì›"],
        "asset_hint": ["ì±„ê¶Œ", "í™˜ìœ¨", "ì£¼ì‹"]
    },
    "ê¸ˆë¦¬/ì—°ì¤€": {
        "kw": ["fed", "fomc", "powell", "rate", "rates", "hike", "cut", "hold", "dot plot", "ì—°ì¤€", "fomc", "íŒŒì›”", "ê¸°ì¤€ê¸ˆë¦¬", "ê¸ˆë¦¬ì¸ìƒ", "ê¸ˆë¦¬ì¸í•˜", "ë™ê²°"],
        "asset_hint": ["ì±„ê¶Œ", "ì£¼ì‹", "í™˜ìœ¨"]
    },
    "ì±„ê¶Œ/ìˆ˜ìµë¥ ": {
        "kw": ["treasury", "ust", "yield", "10y", "2y", "curve", "spread", "duration", "êµ­ì±„", "ë¯¸êµ­ì±„", "ìˆ˜ìµë¥ ", "ì¼ë“œì»¤ë¸Œ", "ì»¤ë¸Œ", "ìŠ¤í”„ë ˆë“œ", "ë“€ë ˆì´ì…˜"],
        "asset_hint": ["ì±„ê¶Œ"]
    },
    "ë‹¬ëŸ¬/í™˜ìœ¨": {
        "kw": ["dollar", "dxy", "fx", "usd", "usdkrw", "eurusd", "yen", "yuan", "ë‹¬ëŸ¬", "í™˜ìœ¨", "ì›ë‹¬ëŸ¬", "ì™¸í™˜", "ê°•ë‹¬ëŸ¬", "ì•½ë‹¬ëŸ¬"],
        "asset_hint": ["í™˜ìœ¨"]
    },
    "ìœ ê°€/ì—ë„ˆì§€": {
        "kw": ["oil", "wti", "brent", "crude", "opec", "gas", "lng", "ìœ ê°€", "ì›ìœ ", "ì˜¤í™", "ê°ì‚°", "ì¦ì‚°", "ì²œì—°ê°€ìŠ¤", "lng"],
        "asset_hint": ["ì›ìì¬", "ì¸í”Œë ˆ"]
    },
    "ì›ìì¬/ê¸ˆì†": {
        "kw": ["gold", "silver", "copper", "aluminum", "nickel", "lithium", "iron ore", "ê¸ˆ", "ì€", "êµ¬ë¦¬", "ì•Œë£¨ë¯¸ëŠ„", "ë‹ˆì¼ˆ", "ë¦¬íŠ¬", "ì² ê´‘ì„"],
        "asset_hint": ["ì›ìì¬"]
    },
    "ê²½ê¸°/ì„±ì¥": {
        "kw": ["gdp", "growth", "recession", "soft landing", "hard landing", "pmi", "ism", "unemployment", "jobs", "ê³ ìš©", "ì‹¤ì—…", "ê²½ê¸°ì¹¨ì²´", "ì„±ì¥ë¥ ", "pmi", "ism"],
        "asset_hint": ["ì£¼ì‹", "ì±„ê¶Œ"]
    },
    "ì‹¤ì /ì–´ë‹": {
        "kw": ["earnings", "guidance", "revenue", "margin", "eps", "beats", "miss", "ì‹¤ì ", "ì–´ë‹", "ê°€ì´ë˜ìŠ¤", "ë§¤ì¶œ", "ë§ˆì§„", "eps", "ì„œí”„ë¼ì´ì¦ˆ"],
        "asset_hint": ["ì£¼ì‹"]
    },
    "AI/ë°˜ë„ì²´": {
        "kw": ["ai", "gpu", "semiconductor", "chip", "nvidia", "amd", "tsmc", "hbm", "ai", "ë°˜ë„ì²´", "ì¹©", "gpu", "ì—”ë¹„ë””ì•„", "tsmc", "hbm"],
        "asset_hint": ["ì£¼ì‹"]
    },
    "ì¤‘êµ­/ì‹ í¥êµ­": {
        "kw": ["china", "beijing", "yuan", "emerging", "ì¤‘êµ­", "ìœ„ì•ˆ", "ì‹ í¥êµ­", "ë¶€ë™ì‚°", "í—ë‹¤", "ë¶€ì±„"],
        "asset_hint": ["í™˜ìœ¨", "ì›ìì¬", "ì£¼ì‹"]
    },
    "ì§€ì •í•™/ë¦¬ìŠ¤í¬": {
        "kw": ["geopolitical", "sanction", "war", "conflict", "shipping", "strait", "iran", "israel", "ukraine", "ì§€ì •í•™", "ì „ìŸ", "ë¶„ìŸ", "ì œì¬", "í•´ìš´", "í™í•´"],
        "asset_hint": ["ì›ìì¬", "í™˜ìœ¨", "ì£¼ì‹"]
    },
    "ì •ì±…/ê·œì œ": {
        "kw": ["policy", "regulation", "tariff", "ban", "stimulus", "fiscal", "ì •ì±…", "ê·œì œ", "ê´€ì„¸", "ë¶€ì–‘", "ì¬ì •"],
        "asset_hint": ["ì£¼ì‹", "í™˜ìœ¨", "ì±„ê¶Œ"]
    }
}

STOPWORDS_ISSUE = set([
    "the","a","an","and","or","to","of","in","on","for","with","as","at","by",
    "from","after","before","today","live","update","updates",
    "ì‹œì¥","ë¯¸êµ­","ê¸€ë¡œë²Œ","ì´ë²ˆ","ê´€ë ¨","ì†ë³´","ë‹¨ë…","ë¶„ì„","ì „ë§","ê°€ëŠ¥","ìš°ë ¤","ë°œí‘œ"
])

def _norm_text(t: str) -> str:
    t = (t or "").lower()
    t = re.sub(r"<[^>]*>", " ", t)
    t = re.sub(r"[^0-9a-zA-Zê°€-í£\s/\.%\-]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def init_issue_db():
    con = sqlite3.connect(ISSUE_DB_PATH)
    cur = con.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS issue_windows (
        window_start_kst TEXT NOT NULL,
        window_end_kst   TEXT NOT NULL,
        issue            TEXT NOT NULL,
        mention_count    INTEGER NOT NULL,
        top_terms        TEXT,
        PRIMARY KEY (window_start_kst, window_end_kst, issue)
    )
    """)
    cur.execute("""
    CREATE TABLE IF NOT EXISTS issue_articles (
        window_start_kst TEXT NOT NULL,
        window_end_kst   TEXT NOT NULL,
        issue            TEXT NOT NULL,
        title            TEXT,
        link             TEXT,
        published_kst    TEXT,
        source           TEXT
    )
    """)
    con.commit()
    con.close()

@st.cache_resource
def ensure_issue_db():
    init_issue_db()
    return True

def floor_to_30m_kst(dt_kst: datetime) -> datetime:
    m = (dt_kst.minute // 30) * 30
    return dt_kst.replace(minute=m, second=0, microsecond=0)

def score_issue(text: str, issue_name: str) -> int:
    t = _norm_text(text)
    score = 0
    for kw in ISSUES[issue_name]["kw"]:
        k = _norm_text(kw)
        if not k or k in STOPWORDS_ISSUE:
            continue
        if k in t:
            score += 1
    return score

def map_article_to_issue(title: str, summary: str = ""):
    text = f"{title} {summary}"
    t = _norm_text(text)
    if not t or len(t) < 10:
        return None, 0

    best_issue = None
    best_score = 0
    for issue in ISSUES.keys():
        sc = score_issue(t, issue)
        if sc > best_score:
            best_score = sc
            best_issue = issue

    if best_score < 2:
        return None, best_score
    return best_issue, best_score

def fetch_issue_trend_items():
    items = []
    # Yahoo (ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©)
    items.extend(fetch_yahoo_news(["SPY", "QQQ", "^DJI"]))

    # Google RSS (í­ ë„“ê²Œ)
    query = (
        "Fed OR FOMC OR CPI OR inflation OR yields OR dollar OR FX OR "
        "oil OR OPEC OR recession OR GDP OR PMI OR earnings OR guidance OR AI OR semiconductor "
        "when:3d"
    )
    url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=en-US&gl=US&ceid=US:en"
    try:
        feed = feedparser.parse(url)
        for e in feed.entries[:150]:
            title = getattr(e, "title", "")
            link = getattr(e, "link", "")
            if hasattr(e, "published_parsed") and e.published_parsed:
                dt = datetime(*e.published_parsed[:6])
            else:
                dt = datetime.now()
            items.append({
                "title": title,
                "link": link,
                "published_dt": dt,
                "source": e.source.title if hasattr(e, 'source') else "GoogleNews"
            })
    except:
        pass

    # ì¤‘ë³µ ì œê±° + ìµœì‹ ìˆœ
    seen = set()
    uniq = []
    for it in items:
        lk = it.get("link", "")
        if not lk or lk in seen:
            continue
        seen.add(lk)
        uniq.append(it)

    uniq.sort(key=lambda x: x.get("published_dt", datetime.min), reverse=True)
    return uniq

def store_window_issue_stats(ws: str, we: str, issue_counts: dict, issue_top_terms: dict, issue_articles: dict):
    con = sqlite3.connect(ISSUE_DB_PATH)
    cur = con.cursor()

    for issue, cnt in issue_counts.items():
        top_terms = issue_top_terms.get(issue, "")
        cur.execute("""
            INSERT INTO issue_windows(window_start_kst, window_end_kst, issue, mention_count, top_terms)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(window_start_kst, window_end_kst, issue)
            DO UPDATE SET mention_count=excluded.mention_count, top_terms=excluded.top_terms
        """, (ws, we, issue, int(cnt), top_terms))

    cur.execute("""
        DELETE FROM issue_articles
        WHERE window_start_kst=? AND window_end_kst=?
    """, (ws, we))

    for issue, rows in issue_articles.items():
        for r in rows[:10]:
            cur.execute("""
                INSERT INTO issue_articles(window_start_kst, window_end_kst, issue, title, link, published_kst, source)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (ws, we, issue, r.get("title"), r.get("link"), r.get("published_kst"), r.get("source")))

    con.commit()
    con.close()

def read_issue_windows(limit_windows=96):
    con = sqlite3.connect(ISSUE_DB_PATH)
    df = pd.read_sql_query("""
        SELECT window_start_kst, window_end_kst, issue, mention_count, top_terms
        FROM issue_windows
        ORDER BY window_end_kst DESC
        LIMIT ?
    """, con, params=(limit_windows * len(ISSUES),))
    con.close()
    return df

def read_issue_articles(ws: str, we: str, issue: str):
    con = sqlite3.connect(ISSUE_DB_PATH)
    df = pd.read_sql_query("""
        SELECT title, link, published_kst, source
        FROM issue_articles
        WHERE window_start_kst=? AND window_end_kst=? AND issue=?
        ORDER BY published_kst DESC
        LIMIT 20
    """, con, params=(ws, we, issue))
    con.close()
    return df

def compute_current_window_issue_trend():
    ensure_issue_db()

    tz = pytz.timezone("Asia/Seoul")
    now_kst = datetime.now(tz)
    we_dt = floor_to_30m_kst(now_kst)
    ws_dt = we_dt - timedelta(minutes=30)

    ws = ws_dt.strftime("%Y-%m-%d %H:%M")
    we = we_dt.strftime("%Y-%m-%d %H:%M")

    items = fetch_issue_trend_items()

    issue_counts = {k: 0 for k in ISSUES.keys()}
    issue_terms = defaultdict(lambda: defaultdict(int))
    issue_evidence = defaultdict(list)

    for it in items:
        dt = it.get("published_dt")
        if not isinstance(dt, datetime):
            continue

        if dt.tzinfo is None:
            dt_kst = tz.localize(dt)
        else:
            dt_kst = dt.astimezone(tz)

        if not (ws_dt <= dt_kst < we_dt):
            continue

        title = it.get("title", "")
        link = it.get("link", "")
        src = it.get("source", "")

        issue, sc = map_article_to_issue(title, "")
        if issue is None:
            continue

        issue_counts[issue] += 1

        tnorm = _norm_text(title)
        for kw in ISSUES[issue]["kw"]:
            k = _norm_text(kw)
            if k and k in tnorm and k not in STOPWORDS_ISSUE:
                issue_terms[issue][k] += 1

        issue_evidence[issue].append({
            "title": title,
            "link": link,
            "published_kst": dt_kst.strftime("%Y-%m-%d %H:%M"),
            "source": src
        })

    issue_top_terms = {}
    for issue, d in issue_terms.items():
        top = sorted(d.items(), key=lambda x: x[1], reverse=True)[:5]
        issue_top_terms[issue] = ", ".join([k for k, v in top])

    store_window_issue_stats(ws, we, issue_counts, issue_top_terms, issue_evidence)
    return ws, we

def build_issue_rank(df_all: pd.DataFrame, current_we: str, lookback_windows=48):
    cur = df_all[df_all["window_end_kst"] == current_we].copy()
    if cur.empty:
        return pd.DataFrame()

    df = df_all.copy()
    df["we_dt"] = pd.to_datetime(df["window_end_kst"])
    cur_we_dt = pd.to_datetime(current_we)

    past = df[(df["we_dt"] < cur_we_dt) & (df["we_dt"] >= cur_we_dt - pd.Timedelta(minutes=30*lookback_windows))]

    rows = []
    for issue in ISSUES.keys():
        cur_cnt = int(cur[cur["issue"] == issue]["mention_count"].sum()) if not cur[cur["issue"] == issue].empty else 0
        hist = past[past["issue"] == issue]["mention_count"].astype(float)
        mu = float(hist.mean()) if len(hist) > 0 else 0.0
        sd = float(hist.std(ddof=0)) if len(hist) > 0 else 0.0

        z = (cur_cnt - mu) / (sd + 1e-6) if (len(hist) > 5) else (cur_cnt - mu)
        rows.append([issue, cur_cnt, mu, sd, z])

    out = pd.DataFrame(rows, columns=["issue", "cur_cnt", "mean", "std", "spike_z"])
    out = out.sort_values(["spike_z", "cur_cnt"], ascending=False)

    out["spike_z"] = out["spike_z"].map(lambda x: round(float(x), 2))
    out["mean"] = out["mean"].map(lambda x: round(float(x), 2))
    out["std"] = out["std"].map(lambda x: round(float(x), 2))
    return out



@st.cache_data(ttl=3600)
def fetch_ib_news(bank_name):
    """ì£¼ìš” IBë“¤ì˜ ìµœì‹  ë§ˆì¼“ ì½”ë©˜íŠ¸ ìˆ˜ì§‘ (Google News + Yahoo Finance)"""
    ib_news = []
    
    # 1. Yahoo Finance (í‹°ì»¤ ë§¤í•‘)
    ticker_map = {
        "JP Morgan": "JPM",
        "Goldman Sachs": "GS",
        "Morgan Stanley": "MS"
    }
    
    if bank_name in ticker_map:
        ib_news.extend(fetch_yahoo_news([ticker_map[bank_name]]))

    # 2. Google News RSS
    # ê²€ìƒ‰ì–´ ìµœì í™”: "BankName market outlook 2025" or "BankName stock strategy" relative to last 30 days
    query = f"{bank_name} market outlook strategy forecast when:30d"
    encoded = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(url)
        for e in feed.entries:
            # ë‚ ì§œ íŒŒì‹±
            if hasattr(e, 'published_parsed') and e.published_parsed:
                dt = datetime(*e.published_parsed[:6])
            else:
                dt = datetime.now()

            ib_news.append({
                "title": e.title,
                "link": e.link,
                "published": e.published,
                "published_dt": dt,
                "source": e.source.title if hasattr(e, 'source') else "News"
            })
    except: pass
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    seen_titles = set()
    unique_news = []
    for n in ib_news:
        # ì œëª©ì´ ë„ˆë¬´ ë¹„ìŠ·í•˜ë©´ ì¤‘ë³µ ì²˜ë¦¬ (ê°„ë‹¨í•œ ë¡œì§)
        title_summary = n['title'][:30]
        if title_summary not in seen_titles:
            unique_news.append(n)
            seen_titles.add(title_summary)
            
    # ìµœì‹ ìˆœ ì •ë ¬
    unique_news.sort(key=lambda x: x['published_dt'], reverse=True)
    
    return unique_news[:5] # Top 5

def get_news_tags(title):
    """ë‰´ìŠ¤ ì œëª© ê¸°ë°˜ íƒœê·¸ ìƒì„± (NLP-lite)"""
    title_lower = title.lower()
    tags = []
    
    # 1. Momentum (Positive)
    if any(k in title_lower for k in ["upgrade", "buy", "bull", "overweight", "raise", "top pick", "growth", "positive", "hike"]):
        tags.append(("ğŸš€ Momentum", "#FFEAEA", "#FF0000")) # Text, BG, Color
        
    # 2. Risk (Negative)
    if any(k in title_lower for k in ["downgrade", "sell", "bear", "underweight", "cut", "risk", "warn", "negative", "slow", "recession"]):
        tags.append(("âš ï¸ Risk", "#EAEFFF", "#0000FF"))
        
    # 3. Key Event (Neutral/Impact)
    if any(k in title_lower for k in ["fed", "rate", "cpi", "inflation", "earnings", "policy", "meeting", "tech", "ai "]):
        tags.append(("ğŸ“¢ Event", "#F2F2F2", "#333333"))
        
    return tags

def calculate_super_theme(df, ref_date=None):
    """ìŠˆí¼í…Œë§ˆ ETF ìˆ˜ìµë¥  ê³„ì‚° (FDR ì‚¬ìš©)"""
    results = []
    
    if ref_date is None:
        ref_date = datetime.now()
    
    # FDR ë‚ ì§œ í¬ë§· (YYYY-MM-DD)
    end_date_str = ref_date.strftime("%Y-%m-%d")
    # ì‹œì‘ì¼ì€ ë„‰ë„‰í•˜ê²Œ 2ë‹¬ ì „
    start_date_str = (ref_date - timedelta(days=60)).strftime("%Y-%m-%d")
    
    for i, row in df.iterrows():
        ticker = str(row['Ticker']).strip()
        if ticker.endswith('.KS'): ticker = ticker.replace('.KS', '')
        
        try:
            # FDR ë°ì´í„° ìˆ˜ì§‘ (ê¸°ê°„ ì§€ì •)
            hist = fdr.DataReader(ticker, start_date_str, end_date_str)
            
            if not hist.empty:
                curr = hist['Close'].iloc[-1]
                
                # 1D Return
                if len(hist) >= 2:
                    ret_1d = ((curr - hist['Close'].iloc[-2]) / hist['Close'].iloc[-2]) * 100
                else: ret_1d = 0
                
                # 5D Return
                if len(hist) >= 6:
                    ret_5d = ((curr - hist['Close'].iloc[-6]) / hist['Close'].iloc[-6]) * 100
                else: ret_5d = 0

                # 1M Return (approx 20 trading days)
                if len(hist) >= 21:
                    ret_1m = ((curr - hist['Close'].iloc[-21]) / hist['Close'].iloc[-21]) * 100
                else: 
                    ret_1m = ((curr - hist['Close'].iloc[0]) / hist['Close'].iloc[0]) * 100
                
                results.append({
                    "Ticker": row['Ticker'],
                    "Name": row['Name'],
                    "Theme": row['Theme'],
                    "Price": curr,
                    "1D": round(ret_1d, 2),
                    "5D": round(ret_5d, 2),
                    "1M": round(ret_1m, 2)
                })
            else:
                 st.warning(f"{ticker}: ë°ì´í„° ì—†ìŒ")
        except Exception as e:
            st.error(f"{ticker} ì—ëŸ¬: {e}")
    
    if not results:
        return pd.DataFrame(columns=["Ticker", "Name", "Theme", "Price", "1D", "5D", "1M"])
    
    return pd.DataFrame(results)

def calculate_super_stock(df, ref_date=None):
    """ìŠˆí¼ìŠ¤íƒ ë°ì´í„° ê³„ì‚° (FDR ì‚¬ìš© - í€ë”ë©˜í„¸ ì œì™¸ Price ìœ„ì£¼)"""
    results = []
    
    if ref_date is None:
        ref_date = datetime.now()
        
    end_date_str = ref_date.strftime("%Y-%m-%d")
    start_date_str = (ref_date - timedelta(days=15)).strftime("%Y-%m-%d") # ìŠ¤íƒì€ ì§§ê²Œ ë´„

    for i, row in df.iterrows():
        ticker = str(row['Ticker']).strip()
        if ticker.endswith('.KS'): ticker = ticker.replace('.KS', '')
        
        try:
            hist = fdr.DataReader(ticker, start_date_str, end_date_str)
            
            if not hist.empty:
                curr = hist['Close'].iloc[-1]
                prev = hist['Close'].iloc[-2] if len(hist) >= 2 else curr
                pct = ((curr - prev)/prev)*100 if prev else 0
                
                results.append({
                    "Ticker": row['Ticker'],
                    "Name": row['Name'],
                    "Sector": row['Sector'],
                    "Price": curr,
                    "Change": round(pct, 2),
                    "PER": 0, # N/A
                    "PBR": 0, # N/A
                    "ROE": 0  # N/A
                })
        except: pass
        
    return pd.DataFrame(results)

@st.cache_data(ttl=86400)
def fetch_statcounter_data(metric="search_engine", device="desktop+mobile+tablet+console", region="ww", from_year="2019", from_month="01", to_year=None, to_month=None):
    """StatCounter ë°ì´í„° ìˆ˜ì§‘ (CSV Direct)"""
    import requests
    import io
    from datetime import datetime
    
    # to_year/to_monthê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ê¸°ì¤€
    if to_year is None or to_month is None:
        now = datetime.now()
        to_year = now.year
        to_month = now.month
    
    base_url = "https://gs.statcounter.com/chart.php"
    
    # device íŒŒë¼ë¯¸í„° ì²˜ë¦¬
    # device_hidden ê°’ ì„¤ì • (StatCounterëŠ” device_hiddenì„ ì£¼ë¡œ ì‚¬ìš©)
    device_val = device
    
    # metric ì„¤ì •
    if metric == "search_engine":
        stat_type_hidden = "search_engine"
        stat_type_label = "Search Engine"
    elif metric == "os":
        stat_type_hidden = "os_combined"
        stat_type_label = "OS Market Share"
    elif metric == "browser":
        stat_type_hidden = "browser"
        stat_type_label = "Browser"
        
    params = {
        "device": device, # Label text but utilizing same val for simplicity or need map? 
        # Actually StatCounter url uses 'device' param for label and 'device_hidden' for value.
        # But 'device' param in getting csv might be loose. Let's use correct hidden val.
        "device_hidden": device_val, 
        "multi-device": "true",
        "statType_hidden": stat_type_hidden,
        "region_hidden": region,
        "granularity": "monthly",
        "statType": stat_type_label,
        "region": "Worldwide",
        "fromInt": f"{from_year}{from_month}",
        "toInt": f"{to_year}{to_month:02d}",
        "fromMonthYear": f"{from_year}-{from_month}",
        "toMonthYear": f"{to_year}-{to_month:02d}",
        "csv": "1"
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(base_url, params=params, headers=headers, verify=False)
        if response.status_code == 200:
            df = pd.read_csv(io.StringIO(response.text))
            # ë‚ ì§œë¥¼ YYYY-MM í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m')
            df.set_index('Date', inplace=True)
            return df
        else:
            return pd.DataFrame()
    except Exception as e:
        st.error(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def process_search_engine_data(df):
    """Google, Bing, Yahoo, Other 4íŒŒì „ìœ¼ë¡œ ì •ë¦¬"""
    if df.empty:
        return df
        
    # CSV header might be 'bing' or 'Bing', 'Yahoo!' or 'Yahoo'
    cols = df.columns
    
    # Bing ì´ë¦„ í™•ì¸
    bing_col = 'bing' if 'bing' in cols else 'Bing'
    # Yahoo ì´ë¦„ í™•ì¸
    yahoo_col = 'Yahoo!' if 'Yahoo!' in cols else 'Yahoo'
    
    final_targets = ['Google', bing_col, yahoo_col]
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    valid_targets = [c for c in final_targets if c in cols]
    
    # Other ê³„ì‚°
    other_cols = [c for c in cols if c not in valid_targets]
    
    df_processed = df[valid_targets].copy()
    if other_cols:
        df_processed['Other'] = df[other_cols].sum(axis=1)
    
    # ì´ë¦„ í†µì¼
    rename_map = {}
    if yahoo_col in df_processed.columns:
        rename_map[yahoo_col] = 'Yahoo'
    if bing_col in df_processed.columns:
        rename_map[bing_col] = 'Bing'
        
    if rename_map:
        df_processed.rename(columns=rename_map, inplace=True)
        
    # ìš”ì²­ëœ ìˆœì„œë¡œ ì •ë ¬: Google, Yahoo, Other, Bing
    desired_order = ['Google', 'Yahoo', 'Other', 'Bing']
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§í•˜ì—¬ ìˆœì„œ ì ìš©
    final_order = [c for c in desired_order if c in df_processed.columns]
    
    return df_processed[final_order]

# ë°ì´í„° ë¡œë“œ
macro_metrics, macro_histories = fetch_market_data()

# ---------------------------------------------------------
# 3. ì‚¬ì´ë“œë°” êµ¬ì„±
# ---------------------------------------------------------
with st.sidebar:
    st.title("ğŸŠ Mirae Asset")
    st.subheader("ê³ ê°ìì‚°ë°°ë¶„ë³¸ë¶€")
    st.caption("Strategy Dashboard V4.1")
    st.markdown("---")
    
    menu = st.radio("ë©”ë‰´ ì„ íƒ", [
        "ğŸ“° Daily Market Narrative", 
        "ğŸ“ˆ Super-Stock",
        "ğŸ“Š TIMEFOLIO Analysis"
    ])
    
    if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.rerun()

# ---------------------------------------------------------
# 4. ë©”ì¸ í™”ë©´ ë¡œì§
# ---------------------------------------------------------

# [TAB 1] Daily Market Narrative (ëª¨ë‹ ë¯¸íŒ…ìš©)
if menu == "ğŸ“° Daily Market Narrative":
    st.title("ğŸ“° Daily Market Narrative")
    st.markdown("### â˜• Morning Meeting Board")
    st.info("ì˜¤ëŠ˜ì˜ ì‹œì¥ í™˜ê²½ì„ ì ê²€í•˜ê³ , ìœ ë‹ˆë²„ìŠ¤ í…Œë§ˆì˜ ë¦¬ë°¸ëŸ°ì‹± ì „ëµì„ ë…¼ì˜í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤.")

    # 1. Macro Environment (ì‹œì¥ í™˜ê²½ ì ê²€)
    st.markdown("#### 1. Macro Environment (ì‹œì¥ ë¶„ìœ„ê¸°)")
    cols = st.columns(6)
    
    # í•µì‹¬ ì§€í‘œ ë‚˜ì—´
    indicators = ["KOSPI", "S&P500", "Nasdaq", "USD/KRW", "US 10Y", "WTI Oil"]
    for i, key in enumerate(indicators):
        if key in macro_metrics:
            with cols[i]:
                d = macro_metrics[key]
                color = "normal" if d['pct_change'] >= 0 else "inverse"
                st.metric(key, f"{d['price']:,.2f}", f"{d['pct_change']:.2f}%", delta_color=color)


    st.markdown("---")

    # 1.5 Global Market Event Radar (New Feature)
    st.markdown("#### ğŸš¨ Global Market Event Radar (Key Events)")
    st.info("ğŸŒ ì´ë²ˆ ì£¼ ì‹œì¥ì„ ì›€ì§ì´ëŠ” í•µì‹¬ ë§¤í¬ë¡œ ì´ë²¤íŠ¸ & ë‰´ìŠ¤")
    
    global_events = fetch_global_events()
    if global_events:
        for n in global_events:
            # ë‚ ì§œ í¬ë§·íŒ…
            try:
                dt = datetime.strptime(n['published'], "%a, %d %b %Y %H:%M:%S %Z")
                date_str = dt.strftime("%Y-%m-%d %H:%M")
            except:
                date_str = ""
            
            # íƒœê·¸ ë¶„ì„
            tags = get_news_tags(n['title'])
            tag_html = ""
            for t_text, t_bg, t_col in tags:
                tag_html += f"<span style='background-color:{t_bg}; color:{t_col}; padding: 2px 6px; border-radius: 4px; font-size: 11px; margin-right: 4px; font-weight: bold;'>{t_text}</span>"
            
            # ì¹´ë“œ ìŠ¤íƒ€ì¼ (ì¡°ê¸ˆ ë” ê°•ì¡°ëœ ë””ìì¸)
            st.markdown(f"""
            <div style="padding: 12px; border-left: 4px solid #FF5050; background-color: #fff; box-shadow: 0 1px 3px rgba(0,0,0,0.1); margin-bottom: 10px;">
                <a href="{n['link']}" target="_blank" style="text-decoration: none; color: #111; font-weight: bold; font-size: 15px;">{n['title']}</a>
                <br><div style="margin-top: 6px;">{tag_html} <span style="color: #666; font-size: 12px;">{n['source']} | {date_str}</span></div>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.write("í˜„ì¬ ê°ì§€ëœ ì£¼ìš” ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # =========================
    # Issue Trend UI (KDI-style)
    # =========================
    st.markdown("### ğŸ“ˆ Issue Trend (30ë¶„ ë‹¨ìœ„)")
    with st.expander("ì˜µì…˜", expanded=False):
        refresh_sec = st.selectbox("ìë™ ìƒˆë¡œê³ ì¹¨(ì´ˆ)", [30, 60, 120, 300, 600, 1800], index=3)
        st.caption("30ë¶„ ë‹¨ìœ„ ì§‘ê³„ë¼ ë„ˆë¬´ ì§§ê²Œ ìƒˆë¡œê³ ì¹¨í•  í•„ìš”ëŠ” ì—†ìŒ. ë°ëª¨ìš©ìœ¼ë¡œëŠ” ìœ ìš©í•¨")
        st.markdown(f"<meta http-equiv='refresh' content='{refresh_sec}'>", unsafe_allow_html=True)

    ws, we = compute_current_window_issue_trend()
    df_all = read_issue_windows(limit_windows=96)

    if df_all.empty:
        st.warning("ì´ìŠˆ íŠ¸ë Œë“œ ë°ì´í„° ì—†ìŒ. í˜„ì¬ 30ë¶„ ìœˆë„ìš°ì— ë§¤í•‘ë˜ëŠ” ë‰´ìŠ¤ê°€ ì—†ì„ ìˆ˜ ìˆìŒ.")
    else:
        rank = build_issue_rank(df_all, current_we=we, lookback_windows=48)

        c1, c2 = st.columns([1.1, 0.9])

        with c1:
            st.markdown(f"**í˜„ì¬ ìœˆë„ìš°(KST)**: {ws} ~ {we}")
            st.markdown("**Top Issues (ê¸‰ì¦ z-score ê¸°ì¤€)**")
            show = rank[["issue", "cur_cnt", "spike_z"]].head(10).copy()
            show.columns = ["Issue", "Mentions(í˜„ì¬ 30ë¶„)", "Spike(z)"]
            st.dataframe(show, use_container_width=True)

            default_issue = show.iloc[0]["Issue"] if len(show) > 0 else list(ISSUES.keys())[0]
            issue_sel = st.selectbox("ì´ìŠˆ ì„ íƒ", list(ISSUES.keys()), index=list(ISSUES.keys()).index(default_issue))

        with c2:
            st.markdown("**Trend (ìµœê·¼ 24ì‹œê°„)**")
            tmp = df_all.copy()
            tmp["we_dt"] = pd.to_datetime(tmp["window_end_kst"])
            cur_we_dt = pd.to_datetime(we)
            tmp = tmp[(tmp["we_dt"] <= cur_we_dt) & (tmp["we_dt"] >= cur_we_dt - pd.Timedelta(hours=24))]
            ts = tmp[tmp["issue"] == issue_sel].sort_values("we_dt")[["we_dt", "mention_count"]]

            if ts.empty:
                st.info("í•´ë‹¹ ì´ìŠˆì˜ ìµœê·¼ 24ì‹œê°„ ë°ì´í„°ê°€ ë¶€ì¡±í•¨.")
            else:
                chart_df = ts.rename(columns={"we_dt": "window_end", "mention_count": "mentions"}).set_index("window_end")
                st.line_chart(chart_df)

            cur_row = df_all[(df_all["window_end_kst"] == we) & (df_all["issue"] == issue_sel)]
            top_terms = cur_row["top_terms"].iloc[0] if not cur_row.empty else ""
            st.markdown("**ëŒ€í‘œ í‚¤ì›Œë“œ(í˜„ì¬ ìœˆë„ìš°)**")
            st.write(top_terms if top_terms else "ì—†ìŒ")

        st.markdown("**ê·¼ê±° ê¸°ì‚¬(í˜„ì¬ 30ë¶„)**")
        ev = read_issue_articles(ws, we, issue_sel)
        if ev.empty:
            st.write("ì—†ìŒ")
        else:
            for r in ev.itertuples(index=False):
                title = r.title or "(ì œëª© ì—†ìŒ)"
                link = r.link or ""
                meta = f"{r.published_kst or ''} Â· {r.source or ''}"
                if link:
                    st.markdown(f"- [{title}]({link})  
  {meta}")
                else:
                    st.markdown(f"- {title}  
  {meta}")


    # 2. Global IB House View (ëŒ€ì²´ëœ ê¸°ëŠ¥)
    st.markdown("#### 2. Global IB House View (Wall St. Insight)")
    st.info("ğŸ’¡ ì›”ê°€ ì£¼ìš” íˆ¬ìì€í–‰(IB)ë“¤ì˜ ìµœì‹  ì‹œì¥ ì „ë§ ë° ì „ëµ ë¦¬í¬íŠ¸ ìš”ì•½")

    ib_banks = {
        "JP Morgan": "https://upload.wikimedia.org/wikipedia/commons/thumb/0/07/J_P_Morgan_Chase_Logo_2008_1.svg/1200px-J_P_Morgan_Chase_Logo_2008_1.svg.png",
        "Goldman Sachs": "https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Goldman_Sachs.svg/1200px-Goldman_Sachs.svg.png",
        "Morgan Stanley": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Morgan_Stanley_Logo_1.svg/1200px-Morgan_Stanley_Logo_1.svg.png"
    }
    
    cols = st.columns(3)
    for i, (bank, logo_url) in enumerate(ib_banks.items()):
        with cols[i]:
            st.markdown(f"**ğŸ¦ {bank}**")
            # st.image(logo_url, width=100) # ë¡œê³ ëŠ” ë§í¬ ê¹¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ ìœ ì§€
            
            news = fetch_ib_news(bank)
            if news:
                for n in news:
                    # ë‚ ì§œ í¬ë§·íŒ… ê¹”ë”í•˜ê²Œ
                    try:
                        dt = datetime.strptime(n['published'], "%a, %d %b %Y %H:%M:%S %Z")
                        date_str = dt.strftime("%Y-%m-%d")
                    except:
                        date_str = ""
                    
                    # íƒœê·¸ ë¶„ì„
                    tags = get_news_tags(n['title'])
                    tag_html = ""
                    for t_text, t_bg, t_col in tags:
                        tag_html += f"<span style='background-color:{t_bg}; color:{t_col}; padding: 2px 6px; border-radius: 4px; font-size: 11px; margin-right: 4px; font-weight: bold;'>{t_text}</span>"
                        
                    st.markdown(f"""
                    <div style="padding: 10px; border: 1px solid #e0e0e0; border-radius: 5px; margin-bottom: 10px; background-color: #f9f9f9;">
                        <a href="{n['link']}" target="_blank" style="text-decoration: none; color: #333; font-weight: bold; font-size: 14px;">{n['title']}</a>
                        <br><div style="margin-top: 4px;">{tag_html} <span style="color: #666; font-size: 12px;">{n['source']} | {date_str}</span></div>
                    </div>
                    """, unsafe_allow_html=True)
            else:
                st.caption("ìµœì‹  ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 3. Discussion & Action Plan (íšŒì˜ë¡ ì‘ì„±)
    st.markdown("#### 3. Today's Action Plan (íšŒì˜ ê¸°ë¡)")
    
    c_memo1, c_memo2 = st.columns(2)
    with c_memo1:
        st.text_area("ğŸ—£ï¸ Macro View & Issue", height=150, placeholder="ì˜ˆ: ë¯¸ êµ­ì±„ ê¸ˆë¦¬ ìƒìŠ¹ìœ¼ë¡œ ì¸í•œ ì„±ì¥ì£¼ ì¡°ì • ê°€ëŠ¥ì„± ë…¼ì˜...")
    with c_memo2:
        st.text_area("âš–ï¸ Rebalancing Idea", height=150, placeholder="ì˜ˆ: 'AI ë°˜ë„ì²´' ë¹„ì¤‘ ìœ ì§€í•˜ë˜, '2ì°¨ì „ì§€' ë¹„ì¤‘ ì¶•ì†Œ ì˜ê²¬ ìš°ì„¸...")

    st.markdown("---")

    # 4. Morning Report Helper (New Feature moved here)
    with st.expander("ğŸ“ Morning Report Helper (ë°ì´í„° ë¶„ì„ ë„êµ¬)", expanded=False):
        # ê¸°ì¤€ ë‚ ì§œ ì„ íƒ (ì˜¤ëŠ˜ì´ ê¸°ë³¸)
        col_date, col_dummy = st.columns([1, 2])
        with col_date:
            target_date = st.date_input("ğŸ“… ê¸°ì¤€ ë‚ ì§œ ì„ íƒ (ì´ ë‚ ì§œ ê¸°ì¤€ ìˆ˜ìµë¥  ê³„ì‚°)", datetime.now())

        # í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
        try:
            with open("universe.xlsx", "rb") as f:
                btn = st.download_button(
                    label="ğŸ“¥ ìœ ë‹ˆë²„ìŠ¤ í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ (universe.xlsx)",
                    data=f,
                    file_name="universe.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        except: pass

        # ì…ë ¥ ë°©ì‹ ì„ íƒ
        input_method = st.radio("ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ", ["ğŸ“‚ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ", "âœï¸ í‹°ì»¤ ì§ì ‘ ì…ë ¥ (ë³µì‚¬/ë¶™ì—¬ë„£ê¸°)", "ğŸ ìƒ˜í”Œ ë°ì´í„° (ì‹œì—°ìš©)"], horizontal=True)
        
        df_themes = None
        df_stocks = None
        
        if input_method == "ğŸ“‚ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ":
            uploaded_file = st.file_uploader("universe.xlsx ì—…ë¡œë“œ", type=['xlsx'])
            if uploaded_file:
                try:
                    uploaded_file.seek(0)
                    df_themes = pd.read_excel(uploaded_file, sheet_name=0, engine='openpyxl')
                    try:
                        df_stocks = pd.read_excel(uploaded_file, sheet_name=1, engine='openpyxl')
                    except:
                        df_stocks = None
                    st.success("íŒŒì¼ ë¡œë“œ ì„±ê³µ! (Themes & Stocks)")
                except Exception as e:
                    st.error(f"ì—‘ì…€ ë¡œë“œ ì˜¤ë¥˜ (DRM ë“±): {e}")
                
        elif input_method == "âœï¸ í‹°ì»¤ ì§ì ‘ ì…ë ¥ (ë³µì‚¬/ë¶™ì—¬ë„£ê¸°)":
            c1, c2 = st.columns(2)
            with c1:
                st.markdown("**1. ìŠˆí¼í…Œë§ˆ (ETF)**")
                txt_theme = st.text_area("í‹°ì»¤ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", "396500, LIT, SHLD, 091230", height=100)
                if txt_theme:
                    tickers = [t.strip() for t in txt_theme.split(',')]
                    df_themes = pd.DataFrame({"Ticker": tickers, "Name": tickers, "Theme": ["Manual Input"]*len(tickers)})
            with c2:
                st.markdown("**2. ìŠˆí¼ìŠ¤íƒ (ê°œë³„ì£¼)**")
                txt_stock = st.text_area("í‹°ì»¤ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", "NVDA, AAPL, 005930, MSFT", height=100)
                if txt_stock:
                    tickers = [t.strip() for t in txt_stock.split(',')]
                    df_stocks = pd.DataFrame({"Ticker": tickers, "Name": tickers, "Sector": ["Manual Input"]*len(tickers)})
                    
        elif input_method == "ğŸ ìƒ˜í”Œ ë°ì´í„° (ì‹œì—°ìš©)":
            st.caption("â€» ë°œí‘œ ì‹œì—°ì„ ìœ„í•´ ë¯¸ë¦¬ ì €ì¥ëœ ìœ ë‹ˆë²„ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            # ìƒ˜í”Œ ë°ì´í„° í•˜ë“œì½”ë”©
            theme_data = [["396500", "TIGER ë°˜ë„ì²´", "ë°˜ë„ì²´"], ["LIT", "Global X Lithium", "2ì°¨ì „ì§€"], ["SHLD", "Global X Defense", "ë°©ì‚°"]]
            stock_data = [["NVDA", "Nvidia", "Tech"], ["AAPL", "Apple", "Tech"], ["005930", "Samsung Elec", "Tech"]]
            df_themes = pd.DataFrame(theme_data, columns=["Ticker", "Name", "Theme"])
            df_stocks = pd.DataFrame(stock_data, columns=["Ticker", "Name", "Sector"])
            st.success("ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì¦‰ì‹œ ë¶„ì„ ê°€ëŠ¥)")
        
        # ë¶„ì„ ì‹¤í–‰ UI
        if df_themes is not None or df_stocks is not None:
            t1, t2 = st.tabs(["â–  ìŠˆí¼í…Œë§ˆ (ETF) ê²°ê³¼", "â–  ìŠˆí¼ìŠ¤íƒ (Stock) ê²°ê³¼"])
            
            with t1:
                if df_themes is not None:
                    if st.button("í…Œë§ˆ ë°ì´í„° ê³„ì‚° ì‹œì‘ ğŸš€"):
                        with st.spinner(f"{target_date.strftime('%Y-%m-%d')} ê¸°ì¤€ ìˆ˜ìµë¥  ê³„ì‚° ì¤‘..."):
                            res_theme = calculate_super_theme(df_themes, target_date)
                            
                            def color_val(val):
                                if isinstance(val, (int, float)):
                                    color = 'red' if val > 0 else 'blue' if val < 0 else 'black'
                                    return f'color: {color}'
                                return ''
                            
                            st.dataframe(res_theme.style.map(color_val, subset=['1D', '5D', '1M']), use_container_width=True)
            
            with t2:
                if df_stocks is not None:
                    if st.button("ìŠ¤íƒ ë°ì´í„° ê³„ì‚° ì‹œì‘ ğŸš€"):
                        with st.spinner(f"{target_date.strftime('%Y-%m-%d')} ê¸°ì¤€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                            res_stock = calculate_super_stock(df_stocks, target_date)
                            st.dataframe(res_stock, use_container_width=True)


# [TAB 2] Super-Stock (StatCounter) - íŒ€ì¥ë‹˜ ê°œì¸ ì—…ë¬´
if menu == "ğŸ“ˆ Super-Stock":
    st.header("ğŸ“ˆ Super-Stock (Global Market Share)")
    st.caption("Data Source: StatCounter Global Stats")
    
    # ë©”ì¸ íƒ­ ë¶„ë¦¬: ê²€ìƒ‰ì—”ì§„ vs ëª¨ë°”ì¼ OS
    main_tab1, main_tab2 = st.tabs(["ğŸ” Search Engine War", "ğŸ“± OS Rivalry (Android vs iOS)"])
    
    # [Tab 1] ê²€ìƒ‰ì—”ì§„ (ê¸°ì¡´ ê¸°ëŠ¥)
    with main_tab1:
        st.subheader("Global Search Engine Market Share")
        st.caption("Google vs Bing vs Yahoo vs Other")
        
        sub_tab1, sub_tab2, sub_tab3 = st.tabs(["ğŸ–¥ï¸+ğŸ“± Desktop & Mobile", "ğŸ–¥ï¸ Desktop", "ğŸ“± Mobile"])
        
        # 1. Desktop + Mobile (Combined)
        with sub_tab1:
            df = fetch_statcounter_data("search_engine", device="desktop+mobile+tablet+console")
            df_proc = process_search_engine_data(df)
            
            if not df_proc.empty:
                # ë§‰ëŒ€ ì°¨íŠ¸ (Stacked Bar)
                fig = px.bar(df_proc, title="Search Engine M/S (Total)", barmode='stack', 
                             color_discrete_map={'Google': '#4285F4', 'Bing': '#00A4EF', 'Yahoo': '#7B0099', 'Other': '#999999'})
                
                # Yì¶• ìŠ¤ì¼€ì¼ ì¡°ì • (ë¹„ìœ¨ íŒŒì•… ìš©ì´í•˜ë„ë¡)
                y_min = df_proc['Google'].min() - 5
                if y_min < 0: y_min = 0
                fig.update_layout(yaxis_range=[y_min, 100], legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5))
                
                st.plotly_chart(fig, use_container_width=True)
                st.dataframe(df_proc.sort_index(ascending=False).style.format("{:.1f}%").background_gradient(cmap="Reds", subset=["Google"]), use_container_width=True)

        # 2. Desktop
        with sub_tab2:
            df = fetch_statcounter_data("search_engine", device="desktop")
            df_proc = process_search_engine_data(df)
            
            if not df_proc.empty:
                fig = px.bar(df_proc, title="Search Engine M/S (Desktop)", barmode='stack',
                             color_discrete_map={'Google': '#4285F4', 'Bing': '#00A4EF', 'Yahoo': '#7B0099', 'Other': '#999999'})
                
                y_min = df_proc['Google'].min() - 5
                if y_min < 0: y_min = 0
                fig.update_layout(yaxis_range=[y_min, 100], legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5))

                st.plotly_chart(fig, use_container_width=True)
                st.dataframe(df_proc.sort_index(ascending=False).style.format("{:.1f}%").background_gradient(cmap="Reds", subset=["Google"]), use_container_width=True)

        # 3. Mobile
        with sub_tab3:
            df = fetch_statcounter_data("search_engine", device="mobile")
            df_proc = process_search_engine_data(df)
            
            if not df_proc.empty:
                fig = px.bar(df_proc, title="Search Engine M/S (Mobile)", barmode='stack',
                             color_discrete_map={'Google': '#4285F4', 'Bing': '#00A4EF', 'Yahoo': '#7B0099', 'Other': '#999999'})
                
                y_min = df_proc['Google'].min() - 5
                if y_min < 0: y_min = 0
                fig.update_layout(yaxis_range=[y_min, 100], legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5))

                st.plotly_chart(fig, use_container_width=True)
                st.dataframe(df_proc.sort_index(ascending=False).style.format("{:.1f}%").background_gradient(cmap="Reds", subset=["Google"]), use_container_width=True)

    # [Tab 2] OS Rivalry (New Feature)
    with main_tab2:
        st.subheader("ğŸ“± Mobile & Tablet OS Rivalry (Android vs iOS)")
        st.caption("Which ecosystem is winning? (Data since 2009)")
        
        # ì»¨íŠ¸ë¡¤ íŒ¨ë„
        c1, c2 = st.columns([1, 1])
        with c1:
            os_device = st.radio("Platform", ["Mobile", "Tablet", "Mobile + Tablet"], horizontal=True)
            # íŒŒë¼ë¯¸í„° ë§¤í•‘
            device_param_map = {
                "Mobile": "mobile",
                "Tablet": "tablet",
                "Mobile + Tablet": "mobile+tablet"
            }
            target_device = device_param_map[os_device]
            
        with c2:
            # ì—°ë„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (í˜„ì¬ ì—°ë„ ~ 2009)
            current_year = datetime.now().year
            year_options = [str(y) for y in range(current_year, 2008, -1)]
            period_options = ["Last 12 Months"] + year_options + ["All Time"]
            period = st.selectbox("Period", period_options)
            
        # ë°ì´í„° ìˆ˜ì§‘ (2009ë…„ë¶€í„° ìµœëŒ€ì¹˜)
        # í†µì‹  ì—ëŸ¬ ë°©ì§€ìš© ì˜ˆì™¸ì²˜ë¦¬
        try:
            df_os = fetch_statcounter_data("os", device=target_device, from_year="2009", from_month="01")
        except Exception:
            df_os = pd.DataFrame()
        
        if not df_os.empty:
            # Android, iOS, iPadOS í•„í„°ë§
            targets = ['Android', 'iOS', 'iPadOS']
            # ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸ (ëŒ€ì†Œë¬¸ì ì´ìŠˆ ë°©ì§€)
            valid_targets = []
            rename_map = {}
            for t in targets:
                # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì°¾ê¸°
                for col in df_os.columns:
                    if t.lower() == col.lower():
                        valid_targets.append(col)
                        rename_map[col] = t # í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
                        break
            
            if len(valid_targets) > 0:
                df_final = df_os[valid_targets].copy()
                df_final.rename(columns=rename_map, inplace=True)
                
                # ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (iloc ìŠ¬ë¼ì´ì‹±ì„ ìœ„í•´ í•„ìˆ˜)
                df_final.sort_index(ascending=True, inplace=True)
                
                # ê¸°ê°„ í•„í„°ë§
                if period == "Last 12 Months":
                    df_final = df_final.iloc[-13:] # User Request: 2024-12 ~ 2025-12 (13 months)
                elif period == "All Time":
                    pass
                elif period.isdigit(): # "2025", "2024" etc.
                    df_final = df_final[df_final.index.str.startswith(period)]
                
                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´
                if df_final.empty:
                    st.warning(f"ì„ íƒí•˜ì‹  ê¸°ê°„({period})ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # Tooltip ì •ë ¬ì„ ìœ„í•´ ë§ˆì§€ë§‰ ë°ì´í„° ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì»¬ëŸ¼ ì¬ì •ë ¬
                    # (User Request: ë†’ì´ ìˆëŠ” ìˆ«ìë‘ ì¢…ë¥˜ë¶€í„° ëœ¨ê²Œ)
                    last_row = df_final.iloc[-1]
                    sorted_cols = last_row.sort_values(ascending=False).index.tolist()
                    df_final = df_final[sorted_cols]
                
                # êº¾ì€ì„  ì°¨íŠ¸ (Line Chart)
                # ë°ì´í„° í¬ì¸íŠ¸ê°€ ë§ìœ¼ë©´ ë§ˆì»¤ë¥¼ ìˆ¨ê²¨ì„œ ê¹”ë”í•˜ê²Œ (20ê°œ ë¯¸ë§Œì¼ ë•Œë§Œ í‘œì‹œ)
                show_markers = True if len(df_final) < 20 else False
                
                # ìƒ‰ìƒ ì„¤ì • (User Request: StatCounter Style - Android Orange, iOS Gray)
                colors = {'Android': '#F48024', 'iOS': '#555555', 'iPadOS': '#555555'}
                
                fig = px.line(df_final, title=f"OS Market Share ({os_device}) - {period}", 
                              color_discrete_map=colors,
                              markers=show_markers) 
                
                # ë¼ì¸ ë‘ê»˜ ì„¤ì •
                fig.update_traces(line=dict(width=3))
                
                # ë¼ì¸ ë‘ê»˜ ì„¤ì •
                fig.update_traces(line=dict(width=3))
                
                # Yì¶• & Range Slider ì„¤ì •
                fig.update_layout(
                    # yaxis_range=[0, 100], # ê³ ì • ë²”ìœ„ ì œê±° (Autoë¡œ ì´ë¯¸ì§€ì²˜ëŸ¼ Zoom íš¨ê³¼)
                    yaxis=dict(rangemode='tozero'), # 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ê°•ì œ
                    xaxis=dict(
                        rangeslider=dict(visible=False), # ìš”ì²­ëŒ€ë¡œ ì œê±°
                        type="date"
                    ),
                    legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="center", x=0.5),
                    hovermode="x", # User Request: ìˆ˜ì¹˜ë¥¼ ë”°ë¡œ í‘œì‹œ (Separate)
                    plot_bgcolor='white' # ì´ë¯¸ì§€ì²˜ëŸ¼ ë°°ê²½ ê¹”ë”í•˜ê²Œ
                )
                fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#E5E5E5')
                fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#E5E5E5') # ê²©ì í‘œì‹œ
                
                st.plotly_chart(fig, use_container_width=True)
                
                # ë°ì´í„° í…Œì´ë¸”
                st.markdown("### ğŸ“Š Monthly Data")
                st.dataframe(df_final.sort_index(ascending=False).style.format("{:.1f}%"), use_container_width=True)
            else:
                st.warning("Android ë˜ëŠ” iOS ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            st.error("ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


# [TAB 3] TIMEFOLIO Analysis (ê²½ìŸì‚¬ ë¶„ì„)
if menu == "ğŸ“Š TIMEFOLIO Analysis":
    st.title("ğŸ“Š TIMEFOLIO Official Portfolio & Rebalancing")
    
    etf_categories = {
        "í•´ì™¸ì£¼ì‹í˜• (10ì¢…)": {
            "ê¸€ë¡œë²Œíƒ‘í”½": "22", "ê¸€ë¡œë²Œë°”ì´ì˜¤": "9", "ìš°ì£¼í…Œí¬&ë°©ì‚°": "20",
            "S&P500": "5", "ë‚˜ìŠ¤ë‹¥100": "2", "ê¸€ë¡œë²ŒAI": "6",
            "ì°¨ì´ë‚˜AI": "19", "ë¯¸êµ­ë°°ë‹¹ë‹¤ìš°ì¡´ìŠ¤": "18",
            "ë¯¸êµ­ë‚˜ìŠ¤ë‹¥100ì±„ê¶Œí˜¼í•©50": "10", "ê¸€ë¡œë²Œì†Œë¹„íŠ¸ë Œë“œ": "8"
        },
        "êµ­ë‚´ì£¼ì‹í˜• (7ì¢…)": {
            "Kì‹ ì¬ìƒì—ë„ˆì§€": "16", "Kë°”ì´ì˜¤": "13", "Koreaí”ŒëŸ¬ìŠ¤ë°°ë‹¹": "12",
            "ì½”ìŠ¤í”¼": "11", "ì½”ë¦¬ì•„ë°¸ë¥˜ì—…": "15", "Kì´ë…¸ë² ì´ì…˜": "17", "Kì»¬ì²˜": "1"
        }
    }
    
    c1, c2 = st.columns(2)
    with c1:
        cat = st.selectbox("ë¶„ë¥˜", list(etf_categories.keys()))
    with c2:
        name = st.selectbox("ìƒí’ˆëª…", list(etf_categories[cat].keys()))
    
    target_idx = etf_categories[cat][name]
    
    if st.button("ë°ì´í„° ë¶„ì„ ë° ë¦¬ë°¸ëŸ°ì‹± ìš”ì•½") or st.session_state.get(f"analysis_active_{target_idx}", False):
        st.session_state[f"analysis_active_{target_idx}"] = True

        with st.spinner(f"'{name}' ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # ActiveETFMonitor ì´ˆê¸°í™”
                monitor = ActiveETFMonitor(url=f"https://timefolioetf.co.kr/m11_view.php?idx={target_idx}", etf_name=name)
                
                # ê¸ˆì¼ ë‚ ì§œ (í•œêµ­ ì‹œê°„)
                today = datetime.now(pytz.timezone('Asia/Seoul')).strftime("%Y-%m-%d")
                
                # ê¸ˆì¼ ë°ì´í„° ìˆ˜ì§‘
                df_today = monitor.get_portfolio_data(today)
                monitor.save_data(df_today, today)
                
                # ì „ì¼ ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ í¬ë¡¤ë§)
                try:
                    prev_day = monitor.get_previous_business_day(today)
                    df_prev = monitor.load_data(prev_day)
                    
                    # ë¦¬ë°¸ëŸ°ì‹± ë¶„ì„ ìˆ˜í–‰
                    analysis = monitor.analyze_rebalancing(df_today, df_prev, prev_day, today)
                    analysis_success = True
                except Exception as e:
                    st.warning(f"ì „ì¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¦¬ë°¸ëŸ°ì‹± ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
                    analysis_success = False
                    df_prev = None

                st.success(f"âœ… {name} ë°ì´í„° ë¶„ì„ ì™„ë£Œ" + (f" (ê¸°ì¤€: {today} vs {prev_day})" if analysis_success else ""))

                # --- ë¦¬ë°¸ëŸ°ì‹± ìš”ì•½ (ë¶„ì„ ì„±ê³µ ì‹œ) ---
                if analysis_success:
                    st.subheader("ğŸ”„ ë¦¬ë°¸ëŸ°ì‹± ì •ë°€ ë¶„ì„ (ì‹œì¥ìˆ˜ìµë¥  ì¡°ì • ë°˜ì˜)")
                    
                    # ìš”ì•½ ë©”íŠ¸ë¦­
                    m1, m2, m3, m4 = st.columns(4)
                    m1.metric("ë¹„ì¤‘ í™•ëŒ€", f"{len(analysis['increased_stocks'])} ì¢…ëª©")
                    m2.metric("ë¹„ì¤‘ ì¶•ì†Œ", f"{len(analysis['decreased_stocks'])} ì¢…ëª©")
                    m3.metric("ì‹ ê·œ í¸ì…", f"{len(analysis['new_stocks'])} ì¢…ëª©")
                    m4.metric("ì™„ì „ í¸ì¶œ", f"{len(analysis['removed_stocks'])} ì¢…ëª©")

                    # íƒ­ êµ¬ì„±
                    tab1, tab2, tab3 = st.tabs(["ì£¼ìš” ë³€ê²½ë‚´ì—­", "ì„¸ë¶€ ë³€ë™", "ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤"])
                    
                    with tab1:
                        # ì‹ ê·œ í¸ì… & í¸ì¶œ
                        c1, c2 = st.columns(2)
                        with c1:
                            st.markdown("##### ğŸŸ¢ ì‹ ê·œ í¸ì…")
                            if analysis['new_stocks']:
                                rows = []
                                for s in analysis['new_stocks']:
                                    rows.append({
                                        "ì¢…ëª©ëª…": s['ì¢…ëª©ëª…'],
                                        "í˜„ì¬ë¹„ì¤‘": f"{s['ë¹„ì¤‘_today']:.2f}%",
                                        "ìˆœìˆ˜ë³€ë™": f"+{s['ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”']:.2f}%p"
                                    })
                                st.dataframe(pd.DataFrame(rows), hide_index=True, use_container_width=True)
                            else:
                                st.caption("ì‹ ê·œ í¸ì… ì¢…ëª© ì—†ìŒ")

                        with c2:
                            st.markdown("##### ğŸ”´ ì™„ì „ í¸ì¶œ")
                            if analysis['removed_stocks']:
                                rows = []
                                for s in analysis['removed_stocks']:
                                    rows.append({
                                        "ì¢…ëª©ëª…": s['ì¢…ëª©ëª…'],
                                        "ì´ì „ë¹„ì¤‘": f"{s['ë¹„ì¤‘_prev']:.2f}%",
                                        "ìˆœìˆ˜ë³€ë™": f"{s['ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”']:.2f}%p"
                                    })
                                st.dataframe(pd.DataFrame(rows), hide_index=True, use_container_width=True)
                            else:
                                st.caption("ì™„ì „ í¸ì¶œ ì¢…ëª© ì—†ìŒ")

                    with tab2:
                        # ë¹„ì¤‘ í™•ëŒ€ & ì¶•ì†Œ
                        c1, c2 = st.columns(2)
                        with c1:
                            st.markdown("##### ğŸ”¼ ë¹„ì¤‘ í™•ëŒ€ (Top 5)")
                            if analysis['increased_stocks']:
                                df_inc = pd.DataFrame(analysis['increased_stocks'])
                                df_inc = df_inc.sort_values('ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”', ascending=False).head(5)
                                display_df = df_inc[['ì¢…ëª©ëª…', 'ë¹„ì¤‘_prev', 'ë¹„ì¤‘_today', 'ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”']].copy()
                                display_df.columns = ['ì¢…ëª©ëª…', 'ì´ì „(%)', 'í˜„ì¬(%)', 'ë³€ë™(%p)']
                                st.dataframe(display_df.style.format({'ì´ì „(%)': '{:.2f}', 'í˜„ì¬(%)': '{:.2f}', 'ë³€ë™(%p)': '+{:.2f}'}), hide_index=True, use_container_width=True)
                            else:
                                st.caption("ë¹„ì¤‘ í™•ëŒ€ ì¢…ëª© ì—†ìŒ")

                        with c2:
                            st.markdown("##### ğŸ”½ ë¹„ì¤‘ ì¶•ì†Œ (Top 5)")
                            if analysis['decreased_stocks']:
                                df_dec = pd.DataFrame(analysis['decreased_stocks'])
                                df_dec = df_dec.sort_values('ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”', ascending=True).head(5)
                                display_df = df_dec[['ì¢…ëª©ëª…', 'ë¹„ì¤‘_prev', 'ë¹„ì¤‘_today', 'ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”']].copy()
                                display_df.columns = ['ì¢…ëª©ëª…', 'ì´ì „(%)', 'í˜„ì¬(%)', 'ë³€ë™(%p)']
                                st.dataframe(display_df.style.format({'ì´ì „(%)': '{:.2f}', 'í˜„ì¬(%)': '{:.2f}', 'ë³€ë™(%p)': '{:.2f}'}), hide_index=True, use_container_width=True)
                            else:
                                st.caption("ë¹„ì¤‘ ì¶•ì†Œ ì¢…ëª© ì—†ìŒ")
                                
                        st.info("* **ìˆœìˆ˜ ë³€ë™**: ì‹œì¥ ê°€ê²© ë“±ë½ì— ì˜í•œ 'ê°€ìƒ ë¹„ì¤‘'ì„ ì œì™¸í•œ ë§¤ë‹ˆì €ì˜ ì‹¤ì œ ë§¤ë§¤ë¡œ ì¸í•œ ë¹„ì¤‘ ë³€í™” (ì¶”ì •ì¹˜)")

                    with tab3:
                        st.markdown("##### ğŸ“‹ ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±")
                # ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë° ì°¨íŠ¸
                st.subheader("ğŸ“‹ ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±")
                
                c_chart, c_list = st.columns([1, 1])
                
                with c_chart:
                    # ë„ë„› ì°¨íŠ¸ ë³µì›
                    chart_df = df_today.copy()
                    chart_df['ë¹„ì¤‘'] = pd.to_numeric(chart_df['ë¹„ì¤‘'], errors='coerce')
                    
                    # Top 5 ì™¸ì—ëŠ” 'ê¸°íƒ€'ë¡œ ë¬¶ê¸°
                    chart_df = chart_df.sort_values('ë¹„ì¤‘', ascending=False)
                    if len(chart_df) > 5:
                        top5 = chart_df.iloc[:5]
                        others = chart_df.iloc[5:]
                        others_sum = others['ë¹„ì¤‘'].sum()
                        others_row = pd.DataFrame([{'ì¢…ëª©ëª…': 'ê¸°íƒ€', 'ë¹„ì¤‘': others_sum}])
                        final_chart_df = pd.concat([top5, others_row], ignore_index=True)
                    else:
                        final_chart_df = chart_df

                    fig = px.pie(final_chart_df, values="ë¹„ì¤‘", names="ì¢…ëª©ëª…", hole=0.4, title="í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘", color_discrete_sequence=px.colors.qualitative.Set3)
                    fig.update_traces(textinfo='percent+label')
                    st.plotly_chart(fig, use_container_width=True)
                
                with c_list:
                    # ì „ì²´ ë°ì´í„° í‘œì‹œ (ì‹¬í”Œ í…Œì´ë¸”)
                    df_all = df_today[['ì¢…ëª©ëª…', 'ë¹„ì¤‘']].copy()
                    df_all['ë¹„ì¤‘'] = pd.to_numeric(df_all['ë¹„ì¤‘'], errors='coerce')
                    df_all = df_all.sort_values('ë¹„ì¤‘', ascending=False)
                    
                    # ì¸ë±ìŠ¤ 1ë¶€í„° ì‹œì‘ (ìˆœìœ„)
                    df_all.index = range(1, len(df_all) + 1)
                    
                    # ë¹„ì¤‘ í¬ë§·íŒ…í•˜ì—¬ í‘œì‹œ
                    st.dataframe(df_all.style.format({'ë¹„ì¤‘': '{:.2f}%'}), use_container_width=True)


                # --- [ì‹ ê·œ ê¸°ëŠ¥ 2] ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ---
                st.markdown("---")
                st.subheader("ğŸ“¥ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ")
                
                # ì—‘ì…€ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° í”„ë ˆì„ ì¤€ë¹„
                e_new = pd.DataFrame(analysis['new_stocks']) if analysis['new_stocks'] else pd.DataFrame(columns=['ì¢…ëª©ëª…', 'ë¹„ì¤‘_today', 'ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”'])
                e_inc = pd.DataFrame(analysis['increased_stocks']) if analysis['increased_stocks'] else pd.DataFrame(columns=['ì¢…ëª©ëª…', 'ë¹„ì¤‘_prev', 'ë¹„ì¤‘_today', 'ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”'])
                e_dec = pd.DataFrame(analysis['decreased_stocks']) if analysis['decreased_stocks'] else pd.DataFrame(columns=['ì¢…ëª©ëª…', 'ë¹„ì¤‘_prev', 'ë¹„ì¤‘_today', 'ìˆœìˆ˜_ë¹„ì¤‘ë³€í™”'])
                
                excel_data = to_excel(e_new, e_inc, e_dec, df_today, today)
                
                st.download_button(
                    label="ğŸ“Š ì—‘ì…€ ë¦¬í¬íŠ¸ ë‚´ë ¤ë°›ê¸° (.xlsx)",
                    data=excel_data,
                    file_name=f"{name}_Report_{today}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

                # --- [ì‹ ê·œ ê¸°ëŠ¥ 1] ì¢…ëª© ë¹„ì¤‘ íˆìŠ¤í† ë¦¬ ---
                st.markdown("---")
                st.subheader("ğŸ“… ì¢…ëª© ë¹„ì¤‘ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 30ì¼)")
                
                with st.expander("ğŸ“ˆ ê°œë³„ ì¢…ëª© íŠ¸ë Œë“œ ë¶„ì„ í¼ì¹˜ê¸°", expanded=False):
                    history_df = monitor.load_history(days=30)
                    
                    if not history_df.empty:
                        # ì¢…ëª© ì„ íƒ (Session State í™œìš©í•˜ì—¬ ì„ íƒ ìœ ì§€)
                        all_stocks = sorted(history_df['ì¢…ëª©ëª…'].unique())
                        
                        # Session state í‚¤ ìƒì„±
                        sel_key = "history_selected_stock"
                        if sel_key not in st.session_state:
                            st.session_state[sel_key] = all_stocks[0]
                            
                        # Selectbox with key
                        selected_stock = st.selectbox("ë¶„ì„í•  ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš”", all_stocks, key=sel_key)
                        
                        # ì„ íƒ ì¢…ëª© ë°ì´í„° í•„í„°ë§
                        stock_history = history_df[history_df['ì¢…ëª©ëª…'] == selected_stock].sort_values('ë‚ ì§œ')
                        
                        chart = px.line(stock_history, x='ë‚ ì§œ', y='ë¹„ì¤‘', title=f"{selected_stock} ë¹„ì¤‘ ë³€í™” ì¶”ì´",
                                       markers=True, text='ë¹„ì¤‘')
                        chart.update_traces(textposition="top center")
                        st.plotly_chart(chart, use_container_width=True)
                    else:
                        st.info("ëˆ„ì ëœ íˆìŠ¤í† ë¦¬ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. ë§¤ì¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë©´ ì°¨íŠ¸ê°€ í™œì„±í™”ë©ë‹ˆë‹¤.")
                

            except Exception as e:
                st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.exception(e)

    st.markdown("---")
    st.link_button("ğŸŒ ê³µì‹ ìƒì„¸í˜ì´ì§€ ë°”ë¡œê°€ê¸°", f"https://timefolioetf.co.kr/m11_view.php?idx={target_idx}")